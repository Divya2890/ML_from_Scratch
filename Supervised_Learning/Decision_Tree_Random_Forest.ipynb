{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMo9wlkyAXyN1PI6tWKYS7n"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Implementing Decision Tress"],"metadata":{"id":"htN3ZBIaxnfl"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"kQW4mEp3NaVx","executionInfo":{"status":"ok","timestamp":1724962544117,"user_tz":240,"elapsed":207,"user":{"displayName":"Divya Sree Pulipati","userId":"16558291737148482724"}}},"outputs":[],"source":["import numpy as np\n","from collections import Counter"]},{"cell_type":"code","source":["class Node:\n","  def __init__(self,feature = None,threshold=None ,left= None,right=None,*,value=None):\n","    self.feature = feature\n","    self.threshold = threshold\n","    self.left = left\n","    self.right= right\n","    self.value = value\n","\n","  def is_leaf_node(self):\n","    return self.value is not None\n","\n","class Decision_Tree:\n","  def __init__(self, min_split_samples=2,max_depth=100,n_features=None):\n","    self.min_split_samples = min_split_samples\n","    self.max_depth = max_depth\n","    self.n_features= n_features\n","    self.root = None\n","    # we can also add n_features variable to the decision tree if we decide to use only\n","    # certain no of features and not all the features\n","\n","  def get_leaf_value(self, y):\n","    labels=Counter(y)\n","    value = labels.most_common(1)[0][0]\n","    return value\n","\n","  def get_splits(self,x,feature_id,threshold):\n","    lf_idx,rg_idx = [],[]\n","    for i in x[feature_id]:\n","      if i>threshold:\n","        lf_idx.append()\n","\n","\n","\n","  def entropy(self, y):\n","    hist = np.bincount(y)\n","    p_class = hist/len(y)\n","    return -np.sum([p * np.log2(p) for p in p_class if p>0])\n","\n","  def information_gain(self, x,y, threshold, feature_id):\n","    parent_ent = self.entropy(y)\n","    lf_idx = np.argwhere(x.iloc[:,feature_id] <= threshold).flatten()\n","    rg_idx = np.argwhere(x.iloc[:,feature_id] > threshold).flatten()\n","    if (len(lf_idx) == 0 or len(rg_idx) == 0):\n","            return 0\n","    lf_entropy = self.entropy(y.iloc[lf_idx])\n","    rg_entropy= self.entropy(y.iloc[rg_idx])\n","    gain = parent_ent - (len(lf_idx)*lf_entropy + len(rg_idx)* rg_entropy )/ len(y)\n","    return gain\n","\n","\n","\n","  def best_split(self,x,y,feature_idx):\n","\n","    # go through each features of the training samples and get the unique values of each feature and establish them as a threshold\n","    # and determine the Information gain and find the feature_id and split_threshold for which we get the maximum gain\n","    best_gain = -1\n","    best_feature_id, best_split_threshold = None, None\n","\n","    for feature_id in feature_idx:\n","\n","      x_column = x.iloc[:,feature_id]\n","      thresholds = np.unique(x_column)\n","      for threshold in thresholds:\n","        gain = self.information_gain(x,y,threshold,feature_id)\n","\n","        if gain > best_gain:\n","          best_gain = gain\n","          best_split_threshold = threshold\n","          best_feature_id = feature_id\n","    return best_feature_id, best_split_threshold\n","\n","\n","\n","\n","\n","\n","\n","  def build_tree(self,x,y,depth=0):\n","    n_samples, n_feats = x.shape\n","    n_labels = len(np.unique(y))\n","\n","\n","    # First I will determine the stopping criteria of the tree to determine not to grow the tree any further\n","    if ( depth>=self.max_depth or n_samples <= self.min_split_samples or n_labels == 1 ):\n","      value = self.get_leaf_value(y)\n","      return Node(value = value)\n","\n","    feature_idx = np.random.choice(n_feats,self.n_features,replace = False)\n","\n","    # find the best feature and split threshold\n","\n","    best_feature_id, best_split_threshold = self.best_split(x,y, feature_idx)\n","\n","    # print(\"best feartuer and threshold\",best_feature_id, best_split_threshold)\n","    # create the left and right child nodes\n","\n","    lf_idx = np.argwhere(x.iloc[:,best_feature_id] <= best_split_threshold).flatten()\n","    rg_idx = np.argwhere(x.iloc[:,best_feature_id] > best_split_threshold).flatten()\n","\n","    left = self.build_tree(x.iloc[lf_idx,:],y.iloc[lf_idx],depth+1)\n","    right = self.build_tree(x.iloc[rg_idx,:],y.iloc[rg_idx],depth+1)\n","\n","    return Node(best_feature_id,best_split_threshold, left,right)\n","\n","    # repeat until the stopping criteria meets\n","\n","\n","\n","  def fit(self,x,y):\n","\n","    n_samples, n_feats = x.shape\n","    n_labels = len(np.unique(y))\n","    # also checking the count of the desired features is not more than the available features in the training data\n","    self.n_features = x.shape[1] if not self.n_features else min(self.n_features,x.shape[1])\n","\n","    # I have to build a decision tree for my training\n","    self.root = self.build_tree(x,y)\n","\n","\n","  def traversal_tree(self,x,root):\n","    if (root.is_leaf_node()):\n","      return root.value\n","\n","    if(x[root.feature] <= root.threshold):\n","      # tranverse the left node\n","      return self.traversal_tree(x,root.left)\n","\n","    else:\n","        # traverse the right node\n","        return self.traversal_tree(x,root.right)\n","\n","\n","\n","\n","\n","  def predict(self,x):\n","\n","    # in here we have to tranverse through the already built decision tree from the training data\n","\n","    #y_predict = np.array([self.traversal_tree(row,self.root) for i,row in x.iterrows()])\n","    y_predict = x.apply(lambda row: self.traversal_tree(row, self.root), axis=1)\n","    return y_predict\n","\n","\n","\n"],"metadata":{"id":"JX2gKPJ5Ngru","executionInfo":{"status":"ok","timestamp":1724962546102,"user_tz":240,"elapsed":203,"user":{"displayName":"Divya Sree Pulipati","userId":"16558291737148482724"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["\n","import pandas as pd\n","from sklearn import datasets\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","\n","\n","# Load the breast cancer dataset\n","data = datasets.load_breast_cancer()\n","\n","# Convert the dataset into a pandas DataFrame\n","df = pd.DataFrame(data.data, columns=data.feature_names)\n","\n","# Add the target variable to the DataFrame\n","df['target'] = data.target\n","\n","# Split the data into X (features) and y (target)\n","X = df.drop(columns=['target'])\n","y = df['target']\n","\n","# Split the data into training and testing sets (here we only focus on training set)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","clf = Decision_Tree(max_depth=10,min_split_samples=2)\n","clf.fit(X_train, y_train)\n","predictions = clf.predict(X_test)\n","\n","def accuracy(y_test, y_pred):\n","    return np.sum(y_test == y_pred) / len(y_test)\n","\n","acc = accuracy(y_test, predictions)\n","print(\"Accuracy of the data\",acc)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"FCKGfeQX4BK1","executionInfo":{"status":"ok","timestamp":1724962601162,"user_tz":240,"elapsed":49043,"user":{"displayName":"Divya Sree Pulipati","userId":"16558291737148482724"}},"outputId":"b8275044-3c6f-4732-e06b-457a91a4ed87"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy of the data 0.9473684210526315\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-3-7c6f19dfcbdc>:124: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n","  if(x[root.feature] <= root.threshold):\n"]}]},{"cell_type":"code","source":["from sklearn.tree import DecisionTreeClassifier\n","sk_model = DecisionTreeClassifier(max_depth=10,min_samples_split=2)\n","sk_model.fit(X_train,y_train)\n","sk_predictions = sk_model.predict(X_test)\n","sk_acc = accuracy(y_test, sk_predictions)\n","print(\"Accuracy of the data\",sk_acc)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n48lj5mXyE_3","executionInfo":{"status":"ok","timestamp":1724963147974,"user_tz":240,"elapsed":186,"user":{"displayName":"Divya Sree Pulipati","userId":"16558291737148482724"}},"outputId":"e97fe5a6-6e22-4d7c-c6a6-06527f4f7bc1"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy of the data 0.9298245614035088\n"]}]},{"cell_type":"markdown","source":["# Implementing Random Forest\n","\n","It is the sequential implementation of N decision trees where each decision tree is trained on m samples of the dataset.\n","The final classification output of the model is majority of the class that is predicted from the N decision trees.\n","\n","The hyper parameters of Random Forest on top of the decision tree are N : no of decision trees to be used and selecting m samples that are used to train the model."],"metadata":{"id":"DlUkzQyL5MiQ"}},{"cell_type":"code","source":["from collections import Counter\n","class Random_Forest():\n","  def __init__(self,N=7,m=300):\n","    self.N = N\n","    self.m = m\n","\n","  def fit(self,x,y):\n","    self.model = []\n","    for i in range(0,self.N):\n","      random_state = np.random.randint(0,1000)\n","      random_indices = x.sample(n=self.m, random_state=random_state).index\n","      x_train = x.loc[random_indices]\n","      y_train = y.loc[random_indices]\n","      self.model.append( Decision_Tree())\n","      self.model[i].fit(x_train,y_train)\n","\n","\n","  def predict(self,x):\n","    output = []\n","    y_predictions = []\n","    for i in range(0,self.N):\n","      y_predictions.append(self.model[i].predict(x))\n","    df = pd.DataFrame(y_predictions)\n","    output = df.apply(lambda col : col.value_counts().idxmax(),axis = 0)\n","    return output\n","\n"],"metadata":{"id":"shF49HPJHZCo","executionInfo":{"status":"ok","timestamp":1724962778615,"user_tz":240,"elapsed":368,"user":{"displayName":"Divya Sree Pulipati","userId":"16558291737148482724"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["\n","Rd_fr = Random_Forest()\n","Rd_fr.fit(X_train, y_train)\n","predictions = Rd_fr.predict(X_test)\n","\n","\n","acc = accuracy(y_test, predictions)\n","print(\"Accuracy of the data\",acc)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zOsqsF3f60CP","executionInfo":{"status":"ok","timestamp":1724963012150,"user_tz":240,"elapsed":231252,"user":{"displayName":"Divya Sree Pulipati","userId":"16558291737148482724"}},"outputId":"041fd412-d077-4916-e574-1378800633e7"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-3-7c6f19dfcbdc>:124: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n","  if(x[root.feature] <= root.threshold):\n"]},{"output_type":"stream","name":"stdout","text":["Accuracy of the data 0.956140350877193\n"]}]},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestClassifier\n","\n","sk_rd_fr = RandomForestClassifier(n_estimators=7,max_samples=300)\n","sk_rd_fr.fit(X_train,y_train)\n","sk_predictions = sk_rd_fr.predict(X_test)\n","acc = accuracy(y_test, sk_predictions)\n","print(\"Accuracy of the data\",acc)\n"],"metadata":{"id":"aVA3VK4R_K7_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724963047692,"user_tz":240,"elapsed":178,"user":{"displayName":"Divya Sree Pulipati","userId":"16558291737148482724"}},"outputId":"d289d670-add5-4815-f062-a68ebf8f20c9"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy of the data 0.9649122807017544\n"]}]}]}